"""
Extended version c·ªßa LanguageModelOllama t√≠ch h·ª£p s·∫µn tokenizer 
ƒë·ªÉ t∆∞∆°ng th√≠ch tr·ª±c ti·∫øp v·ªõi MapReduceLLM
"""

import sys
import os
# from llm_mapreduce.mapreduce import MapReduceLLM
# Th√™m path
sys.path.insert(0, '/home/bojjoo/Code/EduAssist')

from api.services.local_llm import LanguageModelOllama
from transformers import AutoTokenizer
from typing import Dict
from api.config import *


class LanguageModelOllamaMapReduce(LanguageModelOllama):
    """
    M·ªü r·ªông LanguageModelOllama ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi MapReduceLLM
    Th√™m tokenizer v√† format l·∫°i output
    
    QUAN TR·ªåNG: Class n√†y override c√°c ph∆∞∆°ng th·ª©c ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi
    MapReduceLLM framework, v·ªën ƒë∆∞·ª£c thi·∫øt k·∫ø cho PyTorch models.
    """
    
    def __init__(self, model: str, tokenizer_name: str = None, 
                 stream: bool = False, temperature: float = 0.7,
                 host: str = "http://localhost:11434", request_timeout: float = 60.0, 
                 max_retries: int = 2):
        """
        Args:
            model: T√™n model Ollama ƒë√£ pull (vd: "shmily_006/Qw3:4b_4bit")
            tokenizer_name: T√™n tokenizer t·ª´ HuggingFace. N·∫øu None, s·∫Ω t·ª± ƒë·ªông ch·ªçn:
                           - Llama models ‚Üí "meta-llama/Llama-3.2-3B" 
                           - Kh√°c ‚Üí "vinai/phobert-base" (ti·∫øng Vi·ªát)
            stream: Stream response hay kh√¥ng
            temperature: Nhi·ªát ƒë·ªô generation
            host: ƒê·ªãa ch·ªâ Ollama server
            request_timeout: Timeout cho request
            max_retries: S·ªë l·∫ßn retry khi l·ªói
        """
        # G·ªçi constructor c·ªßa class cha
        super().__init__(model, stream, temperature, host, request_timeout, max_retries)
        
        # Auto-detect tokenizer n·∫øu kh√¥ng ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
        if tokenizer_name is None:
            tokenizer_name = self._auto_detect_tokenizer(model)
            print(f"üîç Auto-detected tokenizer: {tokenizer_name}")
        
        # Th√™m tokenizer (MapReduce c·∫ßn ƒë·ªÉ ƒë·∫øm tokens)
        print(f"ƒêang t·∫£i tokenizer: {tokenizer_name}...")
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        
        # ƒê·∫∑t pad_token n·∫øu ch∆∞a c√≥ (m·ªôt s·ªë tokenizer kh√¥ng c√≥)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Th√™m thu·ªôc t√≠nh device ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi MapReduce
        # (MapReduce mong ƒë·ª£i PyTorch model, nh∆∞ng ta d√πng Ollama API)
        self.device = "cpu"  # Dummy value v√¨ Ollama ch·∫°y tr√™n server ri√™ng
    
    def _auto_detect_tokenizer(self, model_name: str) -> str:
        """
        T·ª± ƒë·ªông ph√°t hi·ªán tokenizer ph√π h·ª£p d·ª±a tr√™n t√™n model
        
        Args:
            model_name: T√™n model Ollama (vd: "shmily_006/Qw3:4b_4bit", "llama3.2:3b")
            
        Returns:
            T√™n tokenizer HuggingFace ph√π h·ª£p
        """
        model_lower = model_name.lower()
        
        # Qwen models (bao g·ªìm c√°c finetune nh∆∞ Qw3, Qwen, qwen)
        if any(name in model_lower for name in ['qwen', 'qw']):
            # Qwen3 ch∆∞a release ch√≠nh th·ª©c, d√πng Qwen2.5
            if 'qwen3' in model_lower or 'qw3' in model_lower:
                print(f"   ‚ÑπÔ∏è  {model_name} l√† Qwen3-based, d√πng Qwen3-4B tokenizer")
                return "Qwen/Qwen3-4B"
            return "Qwen/Qwen3-4B"
        
        # Llama models
        if 'llama' in model_lower:
            if 'llama3' in model_lower:
                return "meta-llama/Llama-3.2-3B"
            return "meta-llama/Llama-3.2-3B"
        
        # Phi models
        if 'phi' in model_lower:
            return "microsoft/Phi-3-mini-4k-instruct"
        
        # Gemma models
        if 'gemma' in model_lower:
            return "google/gemma-2-2b"
        
        # Mistral models
        if 'mistral' in model_lower or 'mixtral' in model_lower:
            return "mistralai/Mistral-7B-v0.1"
        
        # Default: PhoBERT cho ti·∫øng Vi·ªát
        print(f"   ‚ö†Ô∏è  Kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c model {model_name}, d√πng PhoBERT (ti·∫øng Vi·ªát)")
        return "vinai/phobert-base"
    
    def generate(self, query: str) -> Dict:
        """
        Override ph∆∞∆°ng th·ª©c generate ƒë·ªÉ tr·∫£ v·ªÅ format m√† MapReduce y√™u c·∫ßu
        
        Args:
            query: Prompt/c√¢u h·ªèi
            
        Returns:
            Dict v·ªõi keys: "text", "answer", "rationale"
        """
        # G·ªçi h√†m generate g·ªëc t·ª´ class cha
        response = super().generate(query)
        
        # Format l·∫°i theo y√™u c·∫ßu c·ªßa MapReduce
        return {
            "text": response,
            "answer": response,
            "rationale": f"Generated by Ollama model: {self.model}"
        }
    
    def process_chunk(self, chunk: str, query: str) -> Dict:
        """
        X·ª≠ l√Ω m·ªôt chunk vƒÉn b·∫£n v·ªõi query
        Ph∆∞∆°ng th·ª©c n√†y ƒë∆∞·ª£c g·ªçi b·ªüi MapReduceLLM.map_stage()
        
        Args:
            chunk: ƒêo·∫°n vƒÉn b·∫£n c·∫ßn x·ª≠ l√Ω
            query: C√¢u h·ªèi/y√™u c·∫ßu
            
        Returns:
            Dict v·ªõi keys: "text", "answer", "rationale"
        """
        # T·∫°o prompt k·∫øt h·ª£p query v√† chunk
        # IMPROVED: Th√™m y√™u c·∫ßu model t·ª± generate rationale ƒë·ªÉ t√≠nh confidence ch√≠nh x√°c
        combined_input = f"""{query}

Document:
{chunk}

H√£y tr·∫£ l·ªùi v√† gi·∫£i th√≠ch ƒë·ªô tin c·∫≠y c·ªßa c√¢u tr·∫£ l·ªùi (r√µ r√†ng/ch·∫Øc ch·∫Øn/c√≥ th·ªÉ/suy lu·∫≠n/kh√¥ng c√≥ th√¥ng tin)."""
        
        # Truncate n·∫øu qu√° d√†i (d·ª±a tr√™n tokenizer)
        tokens = self.tokenizer.encode(combined_input)
        max_tokens = getattr(self.tokenizer, 'model_max_length', 4096)
        
        if len(tokens) > max_tokens:
            # C·∫Øt b·ªõt chunk ƒë·ªÉ v·ª´a context window
            truncated_tokens = tokens[:max_tokens]
            combined_input = self.tokenizer.decode(truncated_tokens, skip_special_tokens=True)
        
        # G·ªçi Ollama API
        response = self.generate(combined_input)
        
        return response


# ============= STRUCTURED INFO PROTOCOL & CONFIDENCE CALIBRATION =============

class ConfidenceCalibrator:
    """
    ƒê√°nh gi√° ƒë·ªô tin c·∫≠y c·ªßa c√¢u tr·∫£ l·ªùi d·ª±a tr√™n rationale
    """
    def calibrate_score(self, rationale: str) -> int:
        """
        T√≠nh ƒëi·ªÉm confidence t·ª´ 0-5 d·ª±a tr√™n rationale
        
        Args:
            rationale: L√Ω do/gi·∫£i th√≠ch c·ªßa model
            
        Returns:
            ƒêi·ªÉm tin c·∫≠y t·ª´ 0 (kh√¥ng c√≥ th√¥ng tin) ƒë·∫øn 5 (r·∫•t tin c·∫≠y)
        """
        rationale_lower = rationale.lower()
        
        # Kh√¥ng c√≥ th√¥ng tin
        if any(phrase in rationale_lower for phrase in [
            "no relevant information", "no information", "kh√¥ng c√≥ th√¥ng tin",
            "kh√¥ng li√™n quan", "kh√¥ng t√¨m th·∫•y"
        ]):
            return 0
        
        # ƒê·ªô tin c·∫≠y cao
        if any(phrase in rationale_lower for phrase in [
            "highly supported", "high confidence", "r·∫•t ch·∫Øc ch·∫Øn",
            "ch·∫Øc ch·∫Øn", "r√µ r√†ng", "ƒë∆∞·ª£c n√™u r√µ"
        ]):
            return 5
        
        # ƒê·ªô tin c·∫≠y trung b√¨nh
        if any(phrase in rationale_lower for phrase in [
            "partially", "medium confidence", "c√≥ th·ªÉ", "c√≥ v·∫ª",
            "ph·∫ßn n√†o", "suy lu·∫≠n"
        ]):
            return 3
        
        # M·∫∑c ƒë·ªãnh: ƒë·ªô tin c·∫≠y th·∫•p
        return 1


class StructuredInfoProtocol:
    """
    Protocol chu·∫©n h√≥a th√¥ng tin tr·∫£ v·ªÅ t·ª´ MapReduce
    ƒê·∫£m b·∫£o t·∫•t c·∫£ k·∫øt qu·∫£ tu√¢n theo schema nh·∫•t qu√°n
    """
    def __init__(self, context_window: int = 4096):
        self.calibrator = ConfidenceCalibrator()
        self.context_window = context_window
    
    def format_mapped_result(self, result: Dict) -> Dict:
        """
        Format k·∫øt qu·∫£ t·ª´ map stage theo structured protocol
        
        Args:
            result: Output t·ª´ model v·ªõi keys "text", "answer", "rationale"
            
        Returns:
            Dict theo schema chu·∫©n v·ªõi keys:
            - extracted_info: Th√¥ng tin ƒë∆∞·ª£c r√∫t tr√≠ch
            - rationale: L√Ω do/gi·∫£i th√≠ch
            - answer: C√¢u tr·∫£ l·ªùi
            - confidence_score: ƒêi·ªÉm tin c·∫≠y 0-5
        """
        return {
            "extracted_info": result.get("text", ""),
            "rationale": result.get("rationale", "Generated response"),
            "answer": result.get("answer", "NO INFORMATION"),
            "confidence_score": self.calibrator.calibrate_score(
                result.get("rationale", "")
            )
        }
    
    def chunk_text_if_needed(self, text: str, tokenizer) -> list:
        """
        Chia text th√†nh chunks n·∫øu v∆∞·ª£t qu√° context window
        
        Args:
            text: Text c·∫ßn chia
            tokenizer: Tokenizer ƒë·ªÉ ƒë·∫øm tokens
            
        Returns:
            List of text chunks
        """
        tokens = tokenizer.encode(text)
        if len(tokens) <= self.context_window:
            return [text]
        
        chunks = []
        for i in range(0, len(tokens), self.context_window):
            chunk_tokens = tokens[i:i + self.context_window]
            chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)
            chunks.append(chunk_text)
        
        return chunks
    
    def collapse_results(self, model, group: list, query: str) -> Dict:
        """
        Collapse m·ªôt group c√°c mapped results th√†nh 1 k·∫øt qu·∫£
        
        Args:
            model: Language model instance
            group: List c√°c mapped results c·∫ßn collapse
            query: Query g·ªëc
            
        Returns:
            Collapsed result theo structured format
        """
        # K·∫øt h·ª£p extracted_info t·ª´ t·∫•t c·∫£ items trong group
        combined_text = " ".join(item["extracted_info"] for item in group)
        
        # N·∫øu qu√° d√†i, chunk l·∫°i v√† process t·ª´ng chunk
        chunks = self.chunk_text_if_needed(combined_text, model.tokenizer)
        
        if len(chunks) > 1:
            # Recursive processing cho text d√†i
            collapsed_text = ""
            for chunk in chunks:
                collapse_query = f"{query}\n\nSummarize this information:\n{chunk}"
                result = model.generate(collapse_query)
                collapsed_text += result.get("text", "") + " "
            
            combined_text = collapsed_text.strip()
        
        # Generate final collapsed result
        final_query = f"{query}\n\nConsolidate the following information:\n{combined_text}"
        result = model.generate(final_query)
        
        # Format theo protocol
        return self.format_mapped_result(result)
    
    def reduce_results(self, model, collapsed_results: list, query: str) -> Dict:
        """
        Reduce t·∫•t c·∫£ collapsed results th√†nh c√¢u tr·∫£ l·ªùi cu·ªëi c√πng
        
        Args:
            model: Language model instance
            collapsed_results: List c√°c collapsed results
            query: Query g·ªëc
            
        Returns:
            Final answer theo structured format
        """
        # K·∫øt h·ª£p extracted_info t·ª´ t·∫•t c·∫£ collapsed results
        combined_text = " ".join(item["extracted_info"] for item in collapsed_results)
        
        # N·∫øu qu√° d√†i, chunk v√† process t·ª´ng chunk
        chunks = self.chunk_text_if_needed(combined_text, model.tokenizer)
        
        if len(chunks) > 1:
            # Recursive reduce
            reduced_text = ""
            for chunk in chunks:
                reduce_query = f"{query}\n\nBased on this information:\n{chunk}"
                result = model.generate(reduce_query)
                reduced_text += result.get("text", "") + " "
            
            final_text = reduced_text.strip()
        else:
            final_text = combined_text
        
        # Generate final answer
        final_query = f"{query}\n\nProvide a comprehensive final answer based on:\n{final_text}"
        result = model.generate(final_query)
        
        return {
            "answer": result.get("answer", ""),
            "text": result.get("text", ""),
            "confidence_score": self.calibrator.calibrate_score(
                result.get("rationale", "")
            )
        }


# ============= MAPREDUCE IMPLEMENTATION =============

class OllamaMapReduceLLM:
    """
    Custom MapReduce implementation cho Ollama models
    T∆∞∆°ng th√≠ch v·ªõi th∆∞ vi·ªán llm-mapreduce g·ªëc, bao g·ªìm:
    - StructuredInfoProtocol: Schema chu·∫©n cho output
    - ConfidenceCalibrator: ƒê√°nh gi√° ƒë·ªô tin c·∫≠y
    - Recursive processing: X·ª≠ l√Ω ƒë·ªá quy cho vƒÉn b·∫£n qu√° d√†i
    - Chunk overlap: Gi·ªØ ng·ªØ c·∫£nh gi·ªØa c√°c chunks
    """
    
    def __init__(self, model: LanguageModelOllamaMapReduce, context_window: int = 4096, 
                 collapse_threshold: int = 2048, chunk_overlap: int = 200):
        """
        Args:
            model: Instance c·ªßa LanguageModelOllamaMapReduce
            context_window: K√≠ch th∆∞·ªõc context window
            collapse_threshold: Ng∆∞·ª°ng ƒë·ªÉ collapse chunks
            chunk_overlap: S·ªë tokens overlap gi·ªØa c√°c chunks (default: 200 tokens ~10%)
                          Overlap gi√∫p gi·ªØ ng·ªØ c·∫£nh t·∫°i ranh gi·ªõi chunks
        """
        self.model = model
        self.context_window = context_window
        self.collapse_threshold = collapse_threshold
        self.chunk_overlap = min(chunk_overlap, context_window // 4)  # Max 25% overlap
        
        # Kh·ªüi t·∫°o protocol v√† calibrator (gi·ªëng th∆∞ vi·ªán g·ªëc)
        self.protocol = StructuredInfoProtocol(context_window=context_window)
        self.calibrator = ConfidenceCalibrator()
    
    def chunk_text(self, text: str) -> list:
        """
        Chia vƒÉn b·∫£n th√†nh c√°c chunks v·ªõi overlap
        
        Overlap gi√∫p gi·ªØ ng·ªØ c·∫£nh t·∫°i ranh gi·ªõi chunks, tr√°nh m·∫•t th√¥ng tin
        khi m·ªôt c√¢u ho·∫∑c √Ω t∆∞·ªüng b·ªã c·∫Øt gi·ªØa 2 chunks.
        
        Args:
            text: VƒÉn b·∫£n c·∫ßn chia
            
        Returns:
            List c√°c text chunks v·ªõi overlap
            
        Example:
            context_window=1000, overlap=200
            Chunk 1: tokens[0:1000]
            Chunk 2: tokens[800:1800]  <- overlap 200 tokens
            Chunk 3: tokens[1600:2600] <- overlap 200 tokens
        """
        tokens = self.model.tokenizer.encode(text)
        chunks = []
        
        # T√≠nh step size (context_window - overlap)
        step_size = self.context_window - self.chunk_overlap
        
        i = 0
        while i < len(tokens):
            # L·∫•y chunk v·ªõi size = context_window
            chunk_end = min(i + self.context_window, len(tokens))
            chunk_tokens = tokens[i:chunk_end]
            
            # Decode chunk
            chunk_text = self.model.tokenizer.decode(chunk_tokens, skip_special_tokens=True)
            chunks.append(chunk_text)
            
            # Di chuy·ªÉn index v·ªõi step_size (t·∫°o overlap)
            i += step_size
            
            # N·∫øu ƒë√£ ƒë·∫øn cu·ªëi, break
            if chunk_end == len(tokens):
                break
        
        return chunks
    
    def map_stage(self, chunks: list, query: str) -> list:
        """
        X·ª≠ l√Ω t·ª´ng chunk v√† format theo StructuredInfoProtocol
        
        Args:
            chunks: List c√°c text chunks
            query: Query g·ªëc
            
        Returns:
            List c√°c mapped results theo structured format
        """
        mapped_results = []
        
        for i, chunk in enumerate(chunks):
            print(f"  Processing chunk {i+1}/{len(chunks)}...")
            
            # Process chunk
            result = self.model.process_chunk(chunk, query)
            
            # Format theo structured protocol (gi·ªëng th∆∞ vi·ªán g·ªëc)
            formatted_result = self.protocol.format_mapped_result(result)
            mapped_results.append(formatted_result)
        
        return mapped_results
    
    def collapse_stage(self, mapped_results: list, query: str) -> list:
        """
        G·ªôp c√°c mapped results th√†nh √≠t k·∫øt qu·∫£ h∆°n
        S·ª≠ d·ª•ng StructuredInfoProtocol.collapse_results() (gi·ªëng th∆∞ vi·ªán g·ªëc)
        
        Args:
            mapped_results: List c√°c mapped results
            query: Query g·ªëc
            
        Returns:
            List c√°c collapsed results
        """
        # N·∫øu √≠t k·∫øt qu·∫£, kh√¥ng c·∫ßn collapse
        if len(mapped_results) <= 2:
            return mapped_results
        
        # Group c√°c k·∫øt qu·∫£ theo collapse_threshold
        groups = self._group_chunks(mapped_results)
        
        # Collapse t·ª´ng group b·∫±ng protocol (gi·ªëng th∆∞ vi·ªán g·ªëc)
        collapsed_results = []
        for i, group in enumerate(groups):
            print(f"  Collapsing group {i+1}/{len(groups)}...")
            collapsed_result = self.protocol.collapse_results(self.model, group, query)
            collapsed_results.append(collapsed_result)
        
        return collapsed_results
    
    def _group_chunks(self, mapped_results: list) -> list:
        """
        Group c√°c mapped results theo collapse_threshold
        Logic gi·ªëng group_chunks() trong th∆∞ vi·ªán g·ªëc
        
        Args:
            mapped_results: List c√°c mapped results
            
        Returns:
            List of groups
        """
        groups = []
        current_group = []
        current_length = 0
        
        for result in mapped_results:
            # S·ª≠ d·ª•ng "extracted_info" thay v√¨ "text" (theo protocol)
            text = result.get("extracted_info", "")
            token_length = len(self.model.tokenizer.encode(text))
            
            if current_length + token_length > self.collapse_threshold:
                if current_group:
                    groups.append(current_group)
                current_group = [result]
                current_length = token_length
            else:
                current_group.append(result)
                current_length += token_length
        
        if current_group:
            groups.append(current_group)
        
        return groups
    
    def reduce_stage(self, collapsed_results: list, query: str) -> Dict:
        """
        T·ªïng h·ª£p k·∫øt qu·∫£ cu·ªëi c√πng
        S·ª≠ d·ª•ng StructuredInfoProtocol.reduce_results() (gi·ªëng th∆∞ vi·ªán g·ªëc)
        
        Args:
            collapsed_results: List c√°c collapsed results
            query: Query g·ªëc
            
        Returns:
            Final result v·ªõi answer v√† confidence_score
        """
        if len(collapsed_results) == 1:
            return collapsed_results[0]
        
        # S·ª≠ d·ª•ng protocol ƒë·ªÉ reduce (gi·ªëng th∆∞ vi·ªán g·ªëc)
        print(f"  Reducing {len(collapsed_results)} collapsed results...")
        final_result = self.protocol.reduce_results(self.model, collapsed_results, query)
        
        return final_result
    
    def process_long_text(self, document: str, query: str) -> Dict:
        """
        X·ª≠ l√Ω vƒÉn b·∫£n d√†i v·ªõi MapReduce
        Pipeline: Chunk ‚Üí Map ‚Üí Collapse ‚Üí Reduce
        
        Args:
            document: VƒÉn b·∫£n d√†i c·∫ßn x·ª≠ l√Ω
            query: C√¢u h·ªèi/y√™u c·∫ßu
            
        Returns:
            Dict v·ªõi keys:
            - answer: C√¢u tr·∫£ l·ªùi cu·ªëi c√πng
            - text: Text ƒë·∫ßy ƒë·ªß
            - confidence_score: ƒêi·ªÉm tin c·∫≠y (n·∫øu c√≥)
        """
        doc_tokens = len(self.model.tokenizer.encode(document))
        print(f"üìÑ Document: {len(document)} chars, {doc_tokens} tokens")
        
        # 1. Chunk vƒÉn b·∫£n v·ªõi overlap
        chunks = self.chunk_text(document)
        print(f"‚úÇÔ∏è  Split into {len(chunks)} chunks (overlap: {self.chunk_overlap} tokens)")
        
        # N·∫øu ch·ªâ c√≥ 1 chunk, x·ª≠ l√Ω tr·ª±c ti·∫øp
        if len(chunks) == 1:
            print("üìù Single chunk, processing directly...")
            result = self.model.process_chunk(chunks[0], query)
            # Format theo protocol
            formatted = self.protocol.format_mapped_result(result)
            return {
                "answer": formatted["answer"],
                "text": formatted["extracted_info"],
                "confidence_score": formatted["confidence_score"]
            }
        
        # 2. Map stage
        print("üó∫Ô∏è  Map stage: Processing chunks...")
        mapped_results = self.map_stage(chunks, query)
        
        # 3. Collapse stage (n·∫øu c·∫ßn)
        if len(mapped_results) > 2:
            print("üîÑ Collapse stage: Merging results...")
            collapsed_results = self.collapse_stage(mapped_results, query)
        else:
            collapsed_results = mapped_results
        
        # 4. Reduce stage
        print("üìä Reduce stage: Final aggregation...")
        final_result = self.reduce_stage(collapsed_results, query)
        
        # ƒê·∫£m b·∫£o return format nh·∫•t qu√°n
        return {
            "answer": final_result.get("answer", final_result.get("text", "")),
            "text": final_result.get("text", final_result.get("answer", "")),
            "confidence_score": final_result.get("confidence_score", 1)
        }


# ============= V√ç D·ª§ S·ª¨ D·ª§NG =============

if __name__ == "__main__":
    ollama_model = LanguageModelOllamaMapReduce(
        model="shmily_006/Qw3:4b_4bit",  # Model custom c·ªßa b·∫°n
        tokenizer_name="Qwen/Qwen3-4B",
        temperature=0.5
    )

    mapreduce_llm = OllamaMapReduceLLM(
        model=ollama_model,
        context_window=4096,  # ƒêi·ªÅu ch·ªânh theo model c·ªßa b·∫°n
        collapse_threshold=2048,
        chunk_overlap=0
    )

    with open("/home/bojjoo/Code/EduAssist/test_data/dienvien Cong Ly.txt") as f:
        long_document = f.read()

    result = mapreduce_llm.process_long_text(long_document, "t√≥m t·∫Øt c√°c √Ω ch√≠nh")
    print(result["answer"])
