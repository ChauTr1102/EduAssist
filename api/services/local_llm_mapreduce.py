"""
Extended version cá»§a LanguageModelOllama tÃ­ch há»£p sáºµn tokenizer 
Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch trá»±c tiáº¿p vá»›i MapReduceLLM
"""

import sys
import os
# from llm_mapreduce.mapreduce import MapReduceLLM
# ThÃªm path
sys.path.insert(0, '/home/bojjoo/Code/EduAssist')

from api.services.local_llm import LanguageModelOllama
from transformers import AutoTokenizer
from typing import Dict


class LanguageModelOllamaMapReduce(LanguageModelOllama):
    """
    Má»Ÿ rá»™ng LanguageModelOllama Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch vá»›i MapReduceLLM
    ThÃªm tokenizer vÃ  format láº¡i output
    
    QUAN TRá»ŒNG: Class nÃ y override cÃ¡c phÆ°Æ¡ng thá»©c Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch vá»›i
    MapReduceLLM framework, vá»‘n Ä‘Æ°á»£c thiáº¿t káº¿ cho PyTorch models.
    """
    
    def __init__(self, model: str, tokenizer_name: str = None, 
                 stream: bool = False, temperature: float = 0.7,
                 host: str = "http://localhost:11434", request_timeout: float = 60.0, 
                 max_retries: int = 2):
        """
        Args:
            model: TÃªn model Ollama Ä‘Ã£ pull (vd: "shmily_006/Qw3:4b_4bit")
            tokenizer_name: TÃªn tokenizer tá»« HuggingFace. Náº¿u None, sáº½ tá»± Ä‘á»™ng chá»n:
                           - Qwen models â†’ "Qwen/Qwen2.5-7B"
                           - Llama models â†’ "meta-llama/Llama-3.2-3B" 
                           - KhÃ¡c â†’ "vinai/phobert-base" (tiáº¿ng Viá»‡t)
            stream: Stream response hay khÃ´ng
            temperature: Nhiá»‡t Ä‘á»™ generation
            host: Äá»‹a chá»‰ Ollama server
            request_timeout: Timeout cho request
            max_retries: Sá»‘ láº§n retry khi lá»—i
        """
        # Gá»i constructor cá»§a class cha
        super().__init__(model, stream, temperature, host, request_timeout, max_retries)
        
        # Auto-detect tokenizer náº¿u khÃ´ng Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh
        if tokenizer_name is None:
            tokenizer_name = self._auto_detect_tokenizer(model)
            print(f"ğŸ” Auto-detected tokenizer: {tokenizer_name}")
        
        # ThÃªm tokenizer (MapReduce cáº§n Ä‘á»ƒ Ä‘áº¿m tokens)
        print(f"Äang táº£i tokenizer: {tokenizer_name}...")
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        
        # Äáº·t pad_token náº¿u chÆ°a cÃ³ (má»™t sá»‘ tokenizer khÃ´ng cÃ³)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # ThÃªm thuá»™c tÃ­nh device Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch vá»›i MapReduce
        # (MapReduce mong Ä‘á»£i PyTorch model, nhÆ°ng ta dÃ¹ng Ollama API)
        self.device = "cpu"  # Dummy value vÃ¬ Ollama cháº¡y trÃªn server riÃªng
    
    def _auto_detect_tokenizer(self, model_name: str) -> str:
        """
        Tá»± Ä‘á»™ng phÃ¡t hiá»‡n tokenizer phÃ¹ há»£p dá»±a trÃªn tÃªn model
        
        Args:
            model_name: TÃªn model Ollama (vd: "shmily_006/Qw3:4b_4bit", "llama3.2:3b")
            
        Returns:
            TÃªn tokenizer HuggingFace phÃ¹ há»£p
        """
        model_lower = model_name.lower()
        
        # Llama models
        if 'llama' in model_lower:
            if 'llama3' in model_lower:
                return "meta-llama/Llama-3.2-3B"
            return "meta-llama/Llama-3.2-3B"
        
        # Phi models
        if 'phi' in model_lower:
            return "microsoft/Phi-3-mini-4k-instruct"
        
        # Gemma models
        if 'gemma' in model_lower:
            return "google/gemma-2-2b"
        
        # Mistral models
        if 'mistral' in model_lower or 'mixtral' in model_lower:
            return "mistralai/Mistral-7B-v0.1"
        
        # Default: PhoBERT cho tiáº¿ng Viá»‡t
        print(f"   âš ï¸  KhÃ´ng nháº­n diá»‡n Ä‘Æ°á»£c model {model_name}, dÃ¹ng PhoBERT (tiáº¿ng Viá»‡t)")
        return "vinai/phobert-base"
    
    def generate(self, query: str) -> Dict:
        """
        Override phÆ°Æ¡ng thá»©c generate Ä‘á»ƒ tráº£ vá» format mÃ  MapReduce yÃªu cáº§u
        
        Args:
            query: Prompt/cÃ¢u há»i
            
        Returns:
            Dict vá»›i keys: "text", "answer", "rationale"
        """
        # Gá»i hÃ m generate gá»‘c tá»« class cha
        response = super().generate(query)
        
        # Format láº¡i theo yÃªu cáº§u cá»§a MapReduce
        return {
            "text": response,
            "answer": response,
            "rationale": f"Generated by Ollama model: {self.model}"
        }
    
    def process_chunk(self, chunk: str, query: str) -> Dict:
        """
        Xá»­ lÃ½ má»™t chunk vÄƒn báº£n vá»›i query
        PhÆ°Æ¡ng thá»©c nÃ y Ä‘Æ°á»£c gá»i bá»Ÿi MapReduceLLM.map_stage()
        
        Args:
            chunk: Äoáº¡n vÄƒn báº£n cáº§n xá»­ lÃ½
            query: CÃ¢u há»i/yÃªu cáº§u
            
        Returns:
            Dict vá»›i keys: "text", "answer", "rationale"
        """
        # Táº¡o prompt káº¿t há»£p query vÃ  chunk
        combined_input = f"{query}\n\nDocument:\n{chunk}"
        
        # Truncate náº¿u quÃ¡ dÃ i (dá»±a trÃªn tokenizer)
        tokens = self.tokenizer.encode(combined_input)
        max_tokens = getattr(self.tokenizer, 'model_max_length', 4096)
        
        if len(tokens) > max_tokens:
            # Cáº¯t bá»›t chunk Ä‘á»ƒ vá»«a context window
            truncated_tokens = tokens[:max_tokens]
            combined_input = self.tokenizer.decode(truncated_tokens, skip_special_tokens=True)
        
        # Gá»i Ollama API
        response = self.generate(combined_input)
        
        return response


# ============= VÃ Dá»¤ Sá»¬ Dá»¤NG =============

class OllamaMapReduceLLM:
    """
    Custom MapReduce implementation cho Ollama models
    Kháº¯c phá»¥c váº¥n Ä‘á» khÃ´ng tÆ°Æ¡ng thÃ­ch vá»›i MapReduceLLM gá»‘c (thiáº¿t káº¿ cho PyTorch)
    """
    
    def __init__(self, model: LanguageModelOllamaMapReduce, context_window: int = 4096, 
                 collapse_threshold: int = 2048):
        """
        Args:
            model: Instance cá»§a LanguageModelOllamaMapReduce
            context_window: KÃ­ch thÆ°á»›c context window
            collapse_threshold: NgÆ°á»¡ng Ä‘á»ƒ collapse chunks
        """
        self.model = model
        self.context_window = context_window
        self.collapse_threshold = collapse_threshold
    
    def chunk_text(self, text: str) -> list:
        """Chia vÄƒn báº£n thÃ nh cÃ¡c chunks"""
        tokens = self.model.tokenizer.encode(text)
        chunks = []
        
        for i in range(0, len(tokens), self.context_window):
            chunk_tokens = tokens[i:i + self.context_window]
            chunk_text = self.model.tokenizer.decode(chunk_tokens, skip_special_tokens=True)
            chunks.append(chunk_text)
        
        return chunks
    
    def map_stage(self, chunks: list, query: str) -> list:
        """Xá»­ lÃ½ tá»«ng chunk"""
        mapped_results = []
        
        for i, chunk in enumerate(chunks):
            print(f"  Processing chunk {i+1}/{len(chunks)}...")
            result = self.model.process_chunk(chunk, query)
            mapped_results.append(result)
        
        return mapped_results
    
    def collapse_stage(self, mapped_results: list, query: str) -> list:
        """Gá»™p cÃ¡c káº¿t quáº£ láº¡i"""
        # Náº¿u Ã­t káº¿t quáº£, khÃ´ng cáº§n collapse
        if len(mapped_results) <= 2:
            return mapped_results
        
        # Group cÃ¡c káº¿t quáº£
        groups = []
        current_group = []
        current_length = 0
        
        for result in mapped_results:
            text = result.get("text", "")
            token_length = len(self.model.tokenizer.encode(text))
            
            if current_length + token_length > self.collapse_threshold:
                if current_group:
                    groups.append(current_group)
                current_group = [result]
                current_length = token_length
            else:
                current_group.append(result)
                current_length += token_length
        
        if current_group:
            groups.append(current_group)
        
        # Collapse tá»«ng group
        collapsed_results = []
        for group in groups:
            combined_text = " ".join([r["text"] for r in group])
            collapse_query = f"{query}\n\nSummarize the following results:\n{combined_text}"
            
            result = self.model.generate(collapse_query)
            collapsed_results.append(result)
        
        return collapsed_results
    
    def reduce_stage(self, collapsed_results: list, query: str) -> Dict:
        """Tá»•ng há»£p káº¿t quáº£ cuá»‘i cÃ¹ng"""
        if len(collapsed_results) == 1:
            return collapsed_results[0]
        
        # Káº¿t há»£p táº¥t cáº£ káº¿t quáº£
        combined_text = "\n\n".join([r["text"] for r in collapsed_results])
        
        # Truncate náº¿u quÃ¡ dÃ i
        tokens = self.model.tokenizer.encode(combined_text)
        if len(tokens) > self.context_window:
            tokens = tokens[:self.context_window]
            combined_text = self.model.tokenizer.decode(tokens, skip_special_tokens=True)
        
        final_query = f"{query}\n\nBased on these summaries, provide a final comprehensive answer:\n{combined_text}"
        
        final_result = self.model.generate(final_query)
        return final_result
    
    def process_long_text(self, document: str, query: str) -> Dict:
        """
        Xá»­ lÃ½ vÄƒn báº£n dÃ i vá»›i MapReduce
        
        Args:
            document: VÄƒn báº£n dÃ i cáº§n xá»­ lÃ½
            query: CÃ¢u há»i/yÃªu cáº§u
            
        Returns:
            Dict vá»›i keys: "text", "answer", "rationale"
        """
        print(f"ğŸ“„ Document length: {len(document)} chars, {len(self.model.tokenizer.encode(document))} tokens")
        
        # 1. Chunk vÄƒn báº£n
        chunks = self.chunk_text(document)
        print(f"âœ‚ï¸  Split into {len(chunks)} chunks")
        
        # Náº¿u chá»‰ cÃ³ 1 chunk, xá»­ lÃ½ trá»±c tiáº¿p
        if len(chunks) == 1:
            print("ğŸ“ Single chunk, processing directly...")
            return self.model.process_chunk(chunks[0], query)
        
        # 2. Map stage
        print("ğŸ—ºï¸  Map stage: Processing chunks...")
        mapped_results = self.map_stage(chunks, query)
        
        # 3. Collapse stage (náº¿u cáº§n)
        if len(mapped_results) > 2:
            print("ğŸ”„ Collapse stage: Merging results...")
            collapsed_results = self.collapse_stage(mapped_results, query)
        else:
            collapsed_results = mapped_results
        
        # 4. Reduce stage
        print("ğŸ“Š Reduce stage: Final aggregation...")
        final_result = self.reduce_stage(collapsed_results, query)
        
        return final_result


# ============= VÃ Dá»¤ Sá»¬ Dá»¤NG =============

if __name__ == "__main__":
    import asyncio
    
    async def test_mapreduce():
        # 1. Khá»Ÿi táº¡o model - tokenizer sáº½ tá»± Ä‘á»™ng detect
        ollama_model = LanguageModelOllamaMapReduce(
            model="shmily_006/Qw3:4b_4bit",  # Model finetune cá»§a Qwen3
            # tokenizer_name sáº½ tá»± Ä‘á»™ng chá»n Qwen/Qwen2.5-7B
            temperature=0.7
        )
        
        # 2. Khá»Ÿi táº¡o OllamaMapReduceLLM (custom implementation)
        mapreduce_llm = OllamaMapReduceLLM(
            model=ollama_model,
            context_window=2048,
            collapse_threshold=1024
        )
        
        # 3. VÄƒn báº£n dÃ i cáº§n xá»­ lÃ½
        document = """
        Há»™i nghá»‹ láº§n thá»© 8 Ban Cháº¥p hÃ nh Trung Æ°Æ¡ng Äáº£ng khÃ³a XIII Ä‘Ã£ diá»…n ra tá»« ngÃ y 2-5/10/2023.
        
        Há»™i nghá»‹ Ä‘Ã£ tháº£o luáº­n vÃ  quyáº¿t Ä‘á»‹nh nhiá»u váº¥n Ä‘á» quan trá»ng vá» phÃ¡t triá»ƒn kinh táº¿ - xÃ£ há»™i,
        quá»‘c phÃ²ng - an ninh, Ä‘á»‘i ngoáº¡i vÃ  xÃ¢y dá»±ng Äáº£ng. Trung Æ°Æ¡ng Ä‘Ã£ thÃ´ng qua Nghá»‹ quyáº¿t vá»
        tiáº¿p tá»¥c Ä‘áº©y máº¡nh cÃ´ng nghiá»‡p hÃ³a, hiá»‡n Ä‘áº¡i hÃ³a Ä‘áº¥t nÆ°á»›c Ä‘áº¿n nÄƒm 2030, táº§m nhÃ¬n Ä‘áº¿n nÄƒm 2045.
        
        Vá» phÃ¡t triá»ƒn kinh táº¿, Há»™i nghá»‹ nháº¥n máº¡nh cáº§n táº­p trung phÃ¡t triá»ƒn 3 Ä‘á»™t phÃ¡ chiáº¿n lÆ°á»£c:
        hoÃ n thiá»‡n thá»ƒ cháº¿ kinh táº¿ thá»‹ trÆ°á»ng Ä‘á»‹nh hÆ°á»›ng xÃ£ há»™i chá»§ nghÄ©a, phÃ¡t triá»ƒn nguá»“n nhÃ¢n lá»±c,
        vÃ  xÃ¢y dá»±ng háº¡ táº§ng Ä‘á»“ng bá»™, hiá»‡n Ä‘áº¡i. 
        
        Trung Æ°Æ¡ng cÅ©ng Ä‘Ã£ bÃ n vá» cÃ´ng tÃ¡c cÃ¡n bá»™, quyáº¿t Ä‘á»‹nh Ä‘iá»u Ä‘á»™ng, phÃ¢n cÃ´ng má»™t sá»‘ cÃ¡n bá»™
        giá»¯ cÃ¡c chá»©c vá»¥ lÃ£nh Ä‘áº¡o chá»§ chá»‘t cá»§a Äáº£ng vÃ  NhÃ  nÆ°á»›c.
        
        [... ThÃªm nhiá»u ná»™i dung chi tiáº¿t khÃ¡c ...]
        """
        
        # 4. CÃ¢u há»i
        query = "TÃ³m táº¯t cÃ¡c ná»™i dung chÃ­nh cá»§a Há»™i nghá»‹ láº§n thá»© 8 BCH TW Äáº£ng khÃ³a XIII"
        
        # 5. Xá»­ lÃ½ vá»›i MapReduce
        print("\n" + "="*70)
        print("Äang xá»­ lÃ½ vÄƒn báº£n vá»›i MapReduce...")
        print("="*70)
        result = mapreduce_llm.process_long_text(document, query)
        
        print("\n" + "="*70)
        print("Káº¾T QUáº¢:")
        print("="*70)
        print(result["answer"])
        
        # 6. ÄÃ³ng káº¿t ná»‘i
        await ollama_model.aclose()
    
    # Cháº¡y test
    asyncio.run(test_mapreduce())
